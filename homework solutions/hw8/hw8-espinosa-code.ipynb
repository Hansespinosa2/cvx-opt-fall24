{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard form LP barrier method\n",
    "In the following three exercises, you will implement a barrier method for solving the standard\n",
    "form LP\n",
    "\\begin{align}\n",
    "  \\text{minimize} & \\quad c^\\top x \\\\\n",
    "  \\text{subject to} & \\quad Ax = b \\\\\n",
    "  & \\quad  x \\succeq 0\n",
    "\\end{align}\n",
    "with variable $x \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{m \\times n}, m < n$. Throughout this exercise we will\n",
    "assume that A is full rank, and the sublevel sets $\\{x | Ax = b, x \\succeq 0, c^\\top x \\leq \\gamma\\}$ are all\n",
    "bounded. (If this is not the case, the centering problem is unbounded below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering Step\n",
    "Centering step. Implement Newton’s method for solving the centering problem\n",
    "\\begin{align}\n",
    "  \\text{minimize} & \\quad c^\\top x - \\sum_{i=1}^n \\log x_i \\\\\n",
    "  \\text{subject to} & \\quad Ax = b\n",
    "\\end{align}\n",
    "with variable $x$, given a strictly feasible starting point $x_0$. <br> <br>\n",
    "Your code should accept $A,b,c, \\text{ and } x_0$, and return $x^*$, the primal optimal point, $\\nu^*$, a dual optimal point, and the number of Newton steps executed. <br> <br>\n",
    "Use the block elimination method to compute the Newton step. (You can also compute\n",
    "the Newton step via the KKT system, and compare the result to the Newton step\n",
    "computed via block elimination. The two steps should be close, but if any $x_i$\n",
    "is very small, you might get a warning about the condition number of the KKT matrix.) <br> <br>\n",
    "Plot $\\lambda^2/2$ versus iteration $k$, for various problem data and initial points, to verify that\n",
    "your implementation gives asymptotic quadratic convergence. As stopping criterion,\n",
    "you can use $\\lambda^2/2 \\leq 10^{-6}$. Experiment with varying the algorithm parameters $\\alpha$ and $\\beta$,\n",
    "observing the effect on the total number of Newton steps required, for a fixed problem\n",
    "instance. Check that your computed $x^*, \\nu^*$\n",
    "(nearly) satisfy the KKT conditions. <br> <br>\n",
    "To generate some random problem data (i.e., $A, b, c, x_0$), we recommend the following\n",
    "approach. First, generate $A$ randomly. (You might want to check that it has full rank.)\n",
    "Then generate a random positive vector $x_0$, and take $b = Ax_0$. (This ensures that $x_0$\n",
    "is strictly feasible.) The parameter $c$ can be chosen randomly. To be sure the sublevel\n",
    "sets are bounded, you can add a row to $A$ with all positive elements. If you want to\n",
    "be able to repeat a run with the same problem data, be sure to set the state for the\n",
    "uniform and normal random number generators.\n",
    "Here are some hints that may be useful.\n",
    "\n",
    "* We recommend computing $\\lambda^2/2$ using the formula $\\lambda^2 = - \\Delta x_{nt}^\\top \\nabla f(x)$. You don’t really need $\\lambda$ for anything; you can work with $\\lambda^2$ instead. (This is important for reasons described below.)\n",
    "* There can be small numerical errors in the Newton step $\\Delta x_{nt}$ that you compute. When $x$ is nearly optimal, the computed value of $\\lambda$ can actually be (slightly) negative. If you take the squareroot to get $\\lambda$, you’ll get a complex number, and you’ll never recover. Moreover, your line search will never exit. However, this only happens when $x$ is nearly optimal. So if you exit on the condition $\\lambda^2/2 \\leq 10^{-6}$, everything will be fine, even when the computed value of $\\lambda^2$ is negative.\n",
    "* For the line search, you must first multiply the step size $t$ by $\\beta$ until $x + t \\Delta x_{nt}$ is feasible (i.e., strictly positive). If you don’t, when you evaluate $f$ you’ll be taking the logarithm of negative numbers, and you’ll never recover.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100, 200))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Problem dimensions\n",
    "m = 100\n",
    "n = 200\n",
    "\n",
    "#Problem parameters\n",
    "A = np.random.randn(m-1,n) * 100\n",
    "A = np.r_[A,np.random.rand(n).reshape(1,-1)]\n",
    "x_0 = np.random.rand(n)\n",
    "b = A @ x_0\n",
    "c = np.random.rand(n) * 20 + 5\n",
    "\n",
    "b.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 1781.7046045829275\n",
      "g(x) shape:  (200,)\n",
      "H(x) shape:  (200, 200)\n"
     ]
    }
   ],
   "source": [
    "#Functions and derivatives\n",
    "def f(x:np.array, c:np.array):\n",
    "    return c.T @ x - np.sum(np.log(x))\n",
    "\n",
    "def g(x:np.array, c:np.array):\n",
    "    return c - 1/x\n",
    "\n",
    "def H(x:np.array,c:np.array = None):\n",
    "    return np.diag((x**(-2)))\n",
    "\n",
    "def H_inv(x: np.array, c:np.array = None):\n",
    "    return np.diag((x**2))\n",
    "\n",
    "print(f\"f(x) = {f(x_0,c)}\")    \n",
    "print(f\"g(x) shape:  {(g(x_0,c)).shape}\")    \n",
    "print(f\"H(x) shape:  {(H(x_0,c)).shape}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1575.3928910349528\n",
      "1499.1873781925447\n",
      "1453.8168760623623\n",
      "1379.0555388936\n",
      "1366.1040049619637\n",
      "1356.1587297101291\n",
      "1350.5842031600969\n",
      "1346.9929879653205\n",
      "1345.1896056344317\n",
      "1343.8789561158512\n",
      "1343.5603612525226\n",
      "1343.5550953898655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/rh2cx23174z28g3pjv50mlrr0000gn/T/ipykernel_15980/2386703098.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  return c.T @ x - np.sum(np.log(x))\n"
     ]
    }
   ],
   "source": [
    "# Backtracking parameters\n",
    "alpha = 0.1\n",
    "beta = 0.3\n",
    "\n",
    "# Stopping Criterion\n",
    "epsilon = 10e-6\n",
    "max_iter = 1000\n",
    "\n",
    "def backtracking_line_search(x, dx, alpha, beta):\n",
    "    t = 1\n",
    "    while (f(x + t * dx,c) > f(x,c) + alpha * t * g(x,c).T @ dx) or np.isnan(f(x + t * dx,c)):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "def newtons_centering_step(A, x_0, b, c, alpha, beta, epsilon, max_iter):\n",
    "    x = x_0\n",
    "    i = 0\n",
    "    while i <= max_iter:\n",
    "        # Compute Newton step and decrement\n",
    "        w = np.linalg.solve((A @ H_inv(x) @ A.T), (-A @ H_inv(x) @ g(x, c)))\n",
    "        dx_nt = - H_inv(x) @ (A.T @ w + g(x, c))\n",
    "        nt_dec_2 = - dx_nt.T @ g(x, c)\n",
    "        \n",
    "        # Stopping Criterion\n",
    "        if nt_dec_2 / 2 <= epsilon:\n",
    "            break\n",
    "\n",
    "        # Line Search\n",
    "        t = backtracking_line_search(x, dx_nt, alpha, beta)\n",
    "        \n",
    "        # Update\n",
    "        x += t * dx_nt\n",
    "\n",
    "        # Iteration update\n",
    "        i += 1\n",
    "\n",
    "newtons_centering_step(A, x_0, b, c, alpha, beta, epsilon, max_iter)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasible Start LP Solver\n",
    "Using the centering code from part (1),\n",
    "implement a barrier method to solve the standard form LP\n",
    "\\begin{align}\n",
    "    \\text{minimize} & \\quad c^\\top x \\\\\n",
    "    \\text{subject to} & \\quad Ax = b \\\\\n",
    "    & \\quad x \\succeq 0\n",
    "\\end{align}\n",
    "with variable $x$, given a strictly feasible starting point $x_0$. Your LP solver should\n",
    "take as argument $A, b, c,$ and $x_0$, and return $x^*$. <br> <br>\n",
    "You can terminate your barrier method when the duality gap, as measured by $n/t$,\n",
    "is smaller than $10^{-3}$. (If you make the tolerance much smaller, you might run into\n",
    "some numerical trouble.) Check your LP solver against the solution found by cvx, for\n",
    "several problem instances. <br> <br>\n",
    "The comments in part (1) on how to generate random data hold here too.\n",
    "Experiment with the parameter $\\mu$ to see the effect on the number of Newton steps per\n",
    "centering step, and the total number of Newton steps required to solve the problem. <br> <br>\n",
    "Plot the progress of the algorithm, for a problem instance with $n = 500$ and $m = 100$,\n",
    "showing duality gap (on a log scale) on the vertical axis, versus the cumulative total\n",
    "number of Newton steps (on a linear scale) on the horizontal axis. <br> <br>\n",
    "Your algorithm should return a $2 \\times k$ matrix history, (where k is the total number\n",
    "of centering steps), whose first row contains the number of Newton steps required\n",
    "for each centering step, and whose second row shows the duality gap at the end of\n",
    "each centering step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LP Solver\n",
    "Using the code from part (2), implement a general standard form LP\n",
    "solver, that takes arguments $A, b, c$, determines (strict) feasibility, and returns an\n",
    "optimal point if the problem is (strictly) feasible. <br> <br>\n",
    "You will need to implement a phase I method, that determines whether the problem\n",
    "is strictly feasible, and if so, finds a strictly feasible point, which can then be fed to\n",
    "the code from part (2). In fact, you can use the code from part (2) to implement the\n",
    "phase I method. <br> <br>\n",
    "To find a strictly feasible initial point x0, we solve the phase I problem\n",
    "\\begin{align}\n",
    "  \\text{minimize} & \\quad t \\\\\n",
    "  \\text{subject to} & \\quad Ax =b \\\\\n",
    "  & \\quad x \\succeq (1-t) \\textbf{1}, \\quad t \\geq 0\n",
    "\\end{align}\n",
    "with variables $x$ and $t$. If we can find a feasible $(x, t)$, with $t < 1$, then $x$ is strictly\n",
    "feasible for the original problem. The converse is also true, so the original LP is strictly\n",
    "feasible if and only if $t^* < 1$, where $t^*$\n",
    "is the optimal value of the phase I problem. <br> <br>\n",
    "We can initialize x and t for the phase I problem with any $x_0$\n",
    "satisfying $Ax_0 = b$, and $t^0 = 2 - \\min_i x_i^0$.(Here we can assume that $\\min x_i^0 \\leq 0$; otherwise $x^0$ is already a strictly feasible point, and we are done.)\n",
    "You can use a change of variable $z= x + (t - 1) \\textbf{1}$ to\n",
    "transform the phase I problem into the form in part (2). <br> <br>\n",
    "Check your LP solver against cvx on several numerical example s, including both feasible and infeasible instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
